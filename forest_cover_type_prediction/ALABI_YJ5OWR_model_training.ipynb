{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":1,"outputs":[{"output_type":"stream","text":"/kaggle/input/itk-2020/x_test.csv\n/kaggle/input/itk-2020/y_train.csv\n/kaggle/input/itk-2020/x_train.csv\n/kaggle/input/itk-2020/y_test_baseline.csv\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n#K Fold CV K=10\nfrom sklearn.model_selection import cross_val_score, train_test_split\nfrom sklearn.metrics import precision_score, recall_score, accuracy_score\nfrom sklearn.model_selection import RandomizedSearchCV\n\nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import StackingClassifier\nfrom xgboost import XGBClassifier\n\nfrom sklearn import metrics\n\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.model_selection import GridSearchCV  #Perforing grid search\n\n\n\nimport matplotlib.pylab as plt\n%matplotlib inline\nfrom matplotlib.pylab import rcParams\nrcParams['figure.figsize'] = 12, 4","execution_count":2,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## helper functions"},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_train_data():\n    forestCoverX = pd.read_csv(\"/kaggle/input/itk-2020/x_train.csv\")\n    forestCoverY = pd.read_csv(\"/kaggle/input/itk-2020/y_train.csv\")\n\n    forestCoverX = forestCoverX.drop(['id'], axis=1)\n    forestCoverY = forestCoverY[\"Cover_Type\"]\n\n\n    forestCoverX = forestCoverX.values\n    forestCoverY = forestCoverY.values\n    \n    return [forestCoverX, forestCoverY]  ","execution_count":3,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def check_model(model, cross_validation_method, forestCoverX, forestCoverY):\n    \n\n    if cross_validation_method == \"holdout\":\n        \n    # Split into train and test sets\n        X_train, X_val, y_train, y_val= train_test_split(forestCoverX,\n                                        forestCoverY,\n                                        test_size=0.20,\n                                        random_state=32)\n\n\n\n        # fit the model with the training data\n        model.fit(X_train,y_train)\n\n\n        # predict the target on the train dataset\n        predict_train = model.predict(X_train)\n\n        # Accuray Score on train dataset\n        accuracy_train = accuracy_score(y_train,predict_train)\n        print('\\n accuracy_score on train dataset : ', accuracy_train)\n\n        # predict the target on the test dataset\n        predict_val = model.predict(X_val)\n\n        # Accuracy Score on test dataset\n        accuracy_val = accuracy_score(y_val,predict_val)\n        print('\\naccuracy_score with hold_out validate dataset : ', accuracy_val)\n    \n    elif cross_validation_method == \"k-fold\":\n        \n        accuracies = cross_val_score(estimator = model, X=forestCoverX, y=forestCoverY, cv=5)\n        print('\\naccuracy_score with k-fold all accuracies : ', accuracies)\n        print('\\naccuracy_score with k-fold mean : ', accuracies.mean())\n    \n    return","execution_count":4,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def fit_model(model,forestCoverX, forestCoverY):\n    \n    model.fit(forestCoverX,forestCoverY)\n    \n    return model","execution_count":5,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def getSubmissionCSV(classifier, csv_name):\n    forestCoverXSubmit = pd.read_csv(\"/kaggle/input/itk-2020/x_test.csv\")\n    forestCoverXSubmit_test = forestCoverXSubmit.drop(['id'], axis=1)\n    \n    forestCoverXSubmit_test = forestCoverXSubmit_test.values\n    \n    y_final = classifier.predict(forestCoverXSubmit_test)\n    \n    index_range = np.arange( forestCoverXSubmit_test.shape[0])\n    data = {'id': index_range.tolist(), 'Cover_Type': y_final.tolist()}\n    \n    output_df = pd.DataFrame(data = data ) \n    output_df.to_csv(csv_name, index=False)","execution_count":6,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Add euclidean distance"},{"metadata":{"trusted":true},"cell_type":"code","source":"# def getEuclidenAndCategoricalTraindata():\n    \n#     forestCoverX = pd.read_csv(\"/kaggle/input/itk-2020/x_train.csv\")\n    \n#     vert_dist = forestCoverX['Vertical_Distance_To_Hydrology'].values\n#     hor_dist = forestCoverX['Horizontal_Distance_To_Hydrology'].values\n\n#     euc_distance = np.sqrt(vert_dist**2 + hor_dist**2)\n\n#     forestCoverX['Euc_Distance_To_Hydrology'] = euc_distance\n    \n    \n#     return forestCoverX","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Remove soil type"},{"metadata":{"trusted":true},"cell_type":"code","source":"# def get_train_data_remove_soil_types():\n#     forestCoverX = pd.read_csv(\"/kaggle/input/itk-2020/x_train.csv\")\n#     forestCoverY = pd.read_csv(\"/kaggle/input/itk-2020/y_train.csv\")\n    \n#     drop_columns = ['id' ,'Soil_Type_1', 'Soil_Type_2', 'Soil_Type_3',\n#        'Soil_Type_4', 'Soil_Type_5', 'Soil_Type_6', 'Soil_Type_7',\n#        'Soil_Type_8', 'Soil_Type_9', 'Soil_Type_10', 'Soil_Type_11',\n#        'Soil_Type_12', 'Soil_Type_13', 'Soil_Type_14', 'Soil_Type_15',\n#        'Soil_Type_16', 'Soil_Type_17', 'Soil_Type_18', 'Soil_Type_19',\n#        'Soil_Type_20', 'Soil_Type_21', 'Soil_Type_22', 'Soil_Type_23',\n#        'Soil_Type_24', 'Soil_Type_25', 'Soil_Type_26', 'Soil_Type_27',\n#        'Soil_Type_28', 'Soil_Type_29', 'Soil_Type_30', 'Soil_Type_31',\n#        'Soil_Type_32', 'Soil_Type_33', 'Soil_Type_34', 'Soil_Type_35',\n#        'Soil_Type_36', 'Soil_Type_37', 'Soil_Type_38', 'Soil_Type_39',\n#        'Soil_Type_40']\n\n#     forestCoverX = forestCoverX.drop(drop_columns, axis=1)\n#     forestCoverY = forestCoverY[\"Cover_Type\"]\n\n\n#     forestCoverX = forestCoverX.values\n#     forestCoverY = forestCoverY.values\n    \n#     return [forestCoverX, forestCoverY]  ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":".................................................................................."},{"metadata":{},"cell_type":"markdown","source":"## KNN"},{"metadata":{"trusted":true},"cell_type":"code","source":"# forestCoverX = getEuclidenAndCategoricalTraindata()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier","execution_count":28,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"forestCoverX, forestCoverY = get_train_data()\n\nknn_classifier = KNeighborsClassifier()\n\ncheck_model(knn_classifier, \"holdout\", forestCoverX, forestCoverY)\n\n#fit knn_classifier\nknn_classifier.fit(forestCoverX, forestCoverY)","execution_count":29,"outputs":[{"output_type":"stream","text":"\n accuracy_score on train dataset :  0.9797243719380636\n\naccuracy_score with hold_out validate dataset :  0.9613975559981314\n","name":"stdout"},{"output_type":"execute_result","execution_count":29,"data":{"text/plain":"KNeighborsClassifier()"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"## multi class logistic regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"# forestCoverX = getEuclidenAndCategoricalTraindata()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\n\nforestCoverX, forestCoverY = get_train_data()\n\nlm = LogisticRegression()\n\ncheck_model(lm, \"holdout\", forestCoverX, forestCoverY)\n\n# lm.fit(forestCoverX, forestCoverY)","execution_count":74,"outputs":[{"output_type":"stream","text":"\n accuracy_score on train dataset :  0.6201846535901109\n\naccuracy_score with hold_out validate dataset :  0.6189914189471614\n","name":"stdout"},{"output_type":"stream","text":"/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n","name":"stderr"}]},{"metadata":{},"cell_type":"markdown","source":"## Decision tree classifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier\n\nforestCoverX, forestCoverY = get_train_data()\n\ndt_classifer = DecisionTreeClassifier()\n\ncheck_model(dt_classifer, \"holdout\", forestCoverX, forestCoverY)","execution_count":76,"outputs":[{"output_type":"stream","text":"\n accuracy_score on train dataset :  1.0\n\naccuracy_score with hold_out validate dataset :  0.9298640308819552\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"# Random Forest"},{"metadata":{"trusted":true},"cell_type":"code","source":"forestCoverX, forestCoverY  =  get_train_data()","execution_count":40,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# forestCoverX = getEuclidenAndCategoricalTraindata()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier","execution_count":41,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rf_classifier = RandomForestClassifier()","execution_count":42,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cv_method = \"holdout\"\ncheck_model(rf_classifier, cv_method, forestCoverX, forestCoverY)","execution_count":43,"outputs":[{"output_type":"stream","text":"\n accuracy_score on train dataset :  1.0\n\naccuracy_score with hold_out validate dataset :  0.9466081483120651\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"## with k fold cross validation"},{"metadata":{"trusted":true},"cell_type":"code","source":"rf_classifier_k_fold = RandomForestClassifier()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cv_method = \"k-fold\"\ncheck_model(rf_classifier_k_fold, cv_method, forestCoverX, forestCoverY)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Get a solution"},{"metadata":{"trusted":true},"cell_type":"code","source":"rf_classier =  fit_model(rf_classifier_k_fold,forestCoverX, forestCoverY)\n\nsolution_name = 'y_test_solution_simple_rf_classifer.csv'\ngetSubmissionCSV(rf_classier, solution_name)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## with parameter check with k-fold validation. "},{"metadata":{},"cell_type":"markdown","source":"Get random grid"},{"metadata":{"trusted":true},"cell_type":"code","source":"n_estimators = [int(x) for x in np.linspace(start = 100, stop = 2000, num = 20)]\n\n# Maximum number of levels in tree\nmax_depth = [int(x) for x in np.linspace(5, 100, num = 20)]\nmax_depth.append(None)\n\n# Minimum number of samples required to split a node\nmin_samples_split = [2, 3, 5, 10]\n\n# Minimum number of samples required at each leaf node\nmin_samples_leaf = [1, 2, 3, 4]\n\n\n# Create the random grid\nrandom_grid = {'n_estimators': n_estimators,\n               'max_depth': max_depth,\n               'min_samples_split': min_samples_split,\n               'min_samples_leaf': min_samples_leaf,\n              }","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"use random_search to get rough area. "},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_samples_for_random_search(sample_size = 50000):\n    row_length = forestCoverX.shape[0]\n\n    idx = np.random.randint(row_length, size=sample_size)\n\n    X_samples =  forestCoverX[idx,:]\n    y_samples =  forestCoverY[idx]\n    \n    return [X_samples, y_samples]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rf = RandomForestClassifier()\n\nX_samples, y_samples = get_samples_for_random_search()\n\n#use 10% of the data, as 400,000 data would take forever, I would use 50,000 randomly selected rows\n\n\n# search across 20 different combinations, and use all available cores\nrf_random = RandomizedSearchCV(estimator = rf, param_distributions \n                               = random_grid, n_iter = 20, cv = 3, \n                               verbose=2, random_state=42, n_jobs = -1)\n\n\nrf_random.fit(X_samples, y_samples)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rf_random.best_params_","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"With this, we narrow the search area down"},{"metadata":{},"cell_type":"markdown","source":"### Grid Search with cross validation on all data"},{"metadata":{"trusted":true},"cell_type":"code","source":"# param_grid  = { 'n_estimators': [2000],\n#                 'max_depth': [50, 60],\n#                'min_samples_split': [2,3,4],\n#                'min_samples_leaf': [1],\n#               }\n\n# rf = RandomForestClassifier()\n\n# grid_search = GridSearchCV(estimator = rf, param_grid = param_grid, \n#                           cv = 3, verbose = 2 , n_jobs = -1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# grid_search.fit(forestCoverX, forestCoverY)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"best_rf =  RandomForestClassifier(n_estimators= 1000,\n             max_depth= 50,\n             min_samples_split= 3,\n                min_samples_leaf= 1 )","execution_count":44,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# forestCoverX = forestCoverX.astype(np.int32)\n# forestCoverY = forestCoverY.astype(np.int32)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cv_method = \"holdout\"\ncheck_model(best_rf, cv_method, forestCoverX, forestCoverY)","execution_count":45,"outputs":[{"output_type":"stream","text":"\n accuracy_score on train dataset :  0.999996926538114\n\naccuracy_score with hold_out validate dataset :  0.9470015490152689\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"## Provide solution and submit"},{"metadata":{"trusted":true},"cell_type":"code","source":"# forestCoverX, forestCoverY = get_train_data()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rf_classifer =  fit_model(best_rf,forestCoverX, forestCoverY)\n\nsolution_name = 'y_test_solution_1000_rf_classifer.csv'\ngetSubmissionCSV(rf_classifer, solution_name)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Get min_max scaled values"},{"metadata":{"trusted":true},"cell_type":"code","source":"forestCoverX = pd.read_csv(\"/kaggle/input/itk-2020/x_train.csv\")\nforestCoverY = pd.read_csv(\"/kaggle/input/itk-2020/y_train.csv\")\n\nforestCoverX = forestCoverX.drop(['id'], axis=1)\nforestCoverY = forestCoverY['Cover_Type']\n\nfrom sklearn.model_selection import train_test_split\n\n#convert to numpy\nforestCoverX = forestCoverX.values\nforestCoverY = forestCoverY.values\n\n\n# Split into train and test sets\nX_train, X_val, y_train, y_val= train_test_split(forestCoverX,\n                                forestCoverY,\n                                test_size=0.20,\n                                random_state=42)\n\nfrom sklearn.preprocessing import MinMaxScaler\n\n# Instantiate MinMaxScaler and use it to rescale X_train and X_vl\nscaler = MinMaxScaler(feature_range=(0, 1))\nrescaledX_train = scaler.fit_transform(X_train)\nrescaledX_val = scaler.fit_transform(X_val)","execution_count":17,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Naive bayes"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.naive_bayes import GaussianNB\n\n# Instantiate a RF classifier\nnb_classifier = GaussianNB()\n\ncheck_model(nb_classifier, \"holdout\", forestCoverX, forestCoverY)","execution_count":30,"outputs":[{"output_type":"stream","text":"\n accuracy_score on train dataset :  0.4605090882267969\n\naccuracy_score with hold_out validate dataset :  0.45770942440559614\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"## SVM"},{"metadata":{"trusted":true},"cell_type":"code","source":"# from sklearn.multiclass import OneVsRestClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.svm import LinearSVC\n\nforestCoverX, forestCoverY = get_train_data()\n\n# Instantiate a multi class SVM. \n# svm_classifier = OneVsRestClassifier(SVC())\nsvm_classifier = LinearSVC()\n\nsvm_classifier.fit(rescaledX_train, y_train)\n\n\nprint(\"Accuracy of random forest classifier Train : \", svm_classifier.score(rescaledX_train, y_train))\n\nprint(\"Accuracy of random forest classifier validation : \", svm_classifier.score(rescaledX_val, y_val))\n","execution_count":18,"outputs":[{"output_type":"stream","text":"Accuracy of random forest classifier Train :  0.7128095744484673\nAccuracy of random forest classifier validation :  0.7130141870128592\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"## Bagging with decision trees"},{"metadata":{"trusted":true},"cell_type":"code","source":"forestCoverX, forestCoverY = get_train_data()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# dtc = DecisionTreeClassifier(criterion=\"entropy\")\n# dtc = DecisionTreeClassifier()\n\nbag_model=BaggingClassifier(base_estimator=dtc, n_estimators=100, bootstrap=True)\n\ncheck_model(bag_model, \"holdout\", forestCoverX, forestCoverY)\n\n# bag_model.fit(forestCoverX,forestCoverY)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cv_method = \"holdout\"\ncheck_model(bag_model, cv_method, forestCoverX, forestCoverY)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"solution_name = 'y_test_solution_1000_bagdtc_classifer.csv'\ngetSubmissionCSV(bag_model, solution_name)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Gradient boosting"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import GradientBoostingClassifier\n\nforestCoverX, forestCoverY = get_train_data()\n\ngradBoostModel = GradientBoostingClassifier ()\n\ncheck_model(gradBoostModel,\"holdout\", forestCoverX, forestCoverY)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## xgboost"},{"metadata":{"trusted":true},"cell_type":"code","source":"forestCoverX, forestCoverY = get_train_data()","execution_count":7,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def check_model_xgb(model, cross_validation_method, forestCoverX, forestCoverY):\n\n\n    if cross_validation_method == \"holdout\":\n        \n    # Split into train and test sets\n        X_train, X_val, y_train, y_val= train_test_split(forestCoverX,\n                                        forestCoverY,\n                                        test_size=0.20,\n                                        random_state=32)\n\n\n\n        # fit the model with the training data\n        model.fit(X_train,y_train)\n\n\n        # predict the target on the train dataset\n        predict_train = model.predict(X_train)\n        predict_train = [round(value) for value in predict_train]\n\n        # Accuray Score on train dataset\n        accuracy_train = accuracy_score(y_train,predict_train)\n        print('\\n accuracy_score on train dataset : ', accuracy_train)\n\n        # predict the target on the test dataset\n        predict_val = model.predict(X_val)\n        predict_val = [round(value) for value in predict_val]\n\n        # Accuracy Score on test dataset\n        accuracy_val = accuracy_score(y_val,predict_val)\n        print('\\naccuracy_score with hold_out validate dataset : ', accuracy_val)\n    \n    elif cross_validation_method == \"k-fold\":\n        \n        accuracies = cross_val_score(estimator = model, X=forestCoverX, y=forestCoverY, cv=5)\n        print('\\naccuracy_score with k-fold all accuracies : ', accuracies)\n        print('\\naccuracy_score with k-fold mean : ', accuracies.mean())\n    \n    return","execution_count":8,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# fit model no training data\nxgb_model = XGBClassifier(n_estimators = 200 , learning_rate = 0.3, max_depth=20, n_jobs = -1)","execution_count":9,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":false},"cell_type":"code","source":"check_model_xgb(xgb_model, \"holdout\", forestCoverX, forestCoverY)","execution_count":13,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xgb_model.fit(forestCoverX,forestCoverY)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def getSubmissionCSV_XGB(classifier, csv_name):\n    forestCoverXSubmit = pd.read_csv(\"/kaggle/input/itk-2020/x_test.csv\")\n    forestCoverXSubmit_test = forestCoverXSubmit.drop(['id'], axis=1)\n    \n    forestCoverXSubmit_test = forestCoverXSubmit_test.values\n    \n    y_final = classifier.predict(forestCoverXSubmit_test)\n    y_final = [round(value) for value in y_final]\n    \n    index_range = np.arange( forestCoverXSubmit_test.shape[0])\n    data = {'id': index_range.tolist(), 'Cover_Type': y_final}\n    \n    output_df = pd.DataFrame(data = data ) \n    output_df.to_csv(csv_name, index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"solution_name = 'y_test_solution_xgb_max_depth_8.csv'\ngetSubmissionCSV_XGB(xgb_model, solution_name)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## without soil types get worse results"},{"metadata":{"trusted":true},"cell_type":"code","source":"# forestCoverX, forestCoverY = get_train_data_remove_soil_types()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# xgb_model = XGBClassifier(n_estimators = 200 , learning_rate = 0.3, max_depth=7)\n\n# xgb_model.fit(forestCoverX,forestCoverY)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# check_model_xgb(xgb_model, \"holdout\", forestCoverX, forestCoverY)","execution_count":12,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Stacking "},{"metadata":{},"cell_type":"markdown","source":"### stacking with KNN, bagging classifier, xgboost, logistic regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"# get a stacking ensemble of models\ndef get_stacking():\n    # define the base models\n    level0 = list()\n    level0.append(('knn', KNeighborsClassifier()))\n    level0.append( ('bag' , BaggingClassifier(base_estimator=DecisionTreeClassifier(criterion=\"entropy\")\n                                                  , n_estimators=100, bootstrap=True) )  )\n    level0.append(('xgb', XGBClassifier(n_estimators = 200 , learning_rate = 0.3, max_depth=20)))\n   \n    level1 = LogisticRegression()\n    # define the stacking ensemble\n    model = StackingClassifier(estimators=level0, final_estimator=level1, cv=5)\n    return model","execution_count":31,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"forestCoverX, forestCoverY = get_train_data()\n\nstack_model = get_stacking()\ncheck_model(stack_model, \"holdout\", forestCoverX, forestCoverY)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"solution_name = 'y_test_solution_stack_classifer.csv'\ngetSubmissionCSV(stack_model, solution_name)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"}},"nbformat":4,"nbformat_minor":4}